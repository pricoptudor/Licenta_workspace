{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from music21 import converter, instrument, note, chord, stream, pitch\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras import Model\n",
    "from keras.layers import Dense, Reshape, InputLayer, Embedding, GlobalAveragePooling1D,\\\n",
    "                            Layer, MultiHeadAttention, LayerNormalization, LSTM, Dropout,\\\n",
    "                            Conv1D, RepeatVector, GRU, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras import Input\n",
    "from keras.losses import CategoricalCrossentropy, binary_crossentropy, categorical_crossentropy\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '0.1',\n",
       " '0.1.2',\n",
       " '0.1.2.3',\n",
       " '0.1.2.3.6.9',\n",
       " '0.1.2.5',\n",
       " '0.1.2.7',\n",
       " '0.1.3.4.6.9',\n",
       " '0.1.3.5.7.8',\n",
       " '0.1.3.7.8',\n",
       " '0.1.4.5',\n",
       " '0.1.4.7',\n",
       " '0.1.5',\n",
       " '0.1.5.6.7',\n",
       " '0.1.5.7',\n",
       " '0.1.5.8',\n",
       " '0.1.6',\n",
       " '0.2',\n",
       " '0.2.3',\n",
       " '0.2.3.6',\n",
       " '0.2.3.7',\n",
       " '0.2.4',\n",
       " '0.2.4.5',\n",
       " '0.2.4.5.7',\n",
       " '0.2.4.5.7.9',\n",
       " '0.2.4.6',\n",
       " '0.2.4.6.7',\n",
       " '0.2.4.6.9',\n",
       " '0.2.4.7',\n",
       " '0.2.4.7.8',\n",
       " '0.2.4.7.9',\n",
       " '0.2.4.8',\n",
       " '0.2.5',\n",
       " '0.2.5.6',\n",
       " '0.2.5.7',\n",
       " '0.2.5.7.8',\n",
       " '0.2.5.8',\n",
       " '0.2.6',\n",
       " '0.2.6.7',\n",
       " '0.2.6.8',\n",
       " '0.2.7',\n",
       " '0.3',\n",
       " '0.3.4',\n",
       " '0.3.5',\n",
       " '0.3.5.7',\n",
       " '0.3.5.8',\n",
       " '0.3.6',\n",
       " '0.3.6.8',\n",
       " '0.3.6.9',\n",
       " '0.3.7',\n",
       " '0.4',\n",
       " '0.4.5',\n",
       " '0.4.5.6',\n",
       " '0.4.5.7',\n",
       " '0.4.6',\n",
       " '0.4.7',\n",
       " '0.4.8',\n",
       " '0.5',\n",
       " '0.5.6',\n",
       " '0.6',\n",
       " '1',\n",
       " '1.2',\n",
       " '1.2.3',\n",
       " '1.2.4',\n",
       " '1.2.4.5.6',\n",
       " '1.2.4.5.6.9',\n",
       " '1.2.4.6',\n",
       " '1.2.4.6.8',\n",
       " '1.2.4.6.9',\n",
       " '1.2.4.7',\n",
       " '1.2.4.8',\n",
       " '1.2.4.8.9',\n",
       " '1.2.5',\n",
       " '1.2.5.8',\n",
       " '1.2.5.9',\n",
       " '1.2.6',\n",
       " '1.2.6.7',\n",
       " '1.2.6.8',\n",
       " '1.2.6.9',\n",
       " '1.2.7',\n",
       " '1.2.7.8',\n",
       " '1.3',\n",
       " '1.3.4',\n",
       " '1.3.4.6',\n",
       " '1.3.4.8',\n",
       " '1.3.5.6.8',\n",
       " '1.3.5.7.10',\n",
       " '1.3.5.8',\n",
       " '1.3.5.8.10',\n",
       " '1.3.5.9',\n",
       " '1.3.6',\n",
       " '1.3.6.7',\n",
       " '1.3.6.8',\n",
       " '1.3.6.9',\n",
       " '1.3.7',\n",
       " '1.3.7.9',\n",
       " '1.3.8',\n",
       " '1.4',\n",
       " '1.4.5',\n",
       " '1.4.5.6',\n",
       " '1.4.6',\n",
       " '1.4.6.7.9',\n",
       " '1.4.6.8',\n",
       " '1.4.6.8.9',\n",
       " '1.4.6.9',\n",
       " '1.4.6.9.10',\n",
       " '1.4.7',\n",
       " '1.4.7.10',\n",
       " '1.4.7.9',\n",
       " '1.4.7.9.10',\n",
       " '1.4.8',\n",
       " '1.5',\n",
       " '1.5.6',\n",
       " '1.5.6.8',\n",
       " '1.5.7',\n",
       " '1.5.8',\n",
       " '1.5.8.9',\n",
       " '1.5.9',\n",
       " '1.6',\n",
       " '1.6.7',\n",
       " '1.7',\n",
       " '10',\n",
       " '10.0',\n",
       " '10.0.1',\n",
       " '10.0.1.3.5.6',\n",
       " '10.0.2',\n",
       " '10.0.2.3',\n",
       " '10.0.2.4',\n",
       " '10.0.2.4.7',\n",
       " '10.0.2.5',\n",
       " '10.0.2.5.7',\n",
       " '10.0.3',\n",
       " '10.0.3.5',\n",
       " '10.0.3.5.6',\n",
       " '10.0.3.6',\n",
       " '10.0.4',\n",
       " '10.0.5',\n",
       " '10.1',\n",
       " '10.1.2',\n",
       " '10.1.2.3',\n",
       " '10.1.3',\n",
       " '10.1.3.4.6',\n",
       " '10.1.3.5',\n",
       " '10.1.3.5.6',\n",
       " '10.1.3.6',\n",
       " '10.1.4',\n",
       " '10.1.4.6',\n",
       " '10.1.5',\n",
       " '10.11',\n",
       " '10.11.0',\n",
       " '10.11.0.1',\n",
       " '10.11.0.1.4',\n",
       " '10.11.1',\n",
       " '10.11.1.3.6',\n",
       " '10.11.1.4',\n",
       " '10.11.1.4.6',\n",
       " '10.11.2',\n",
       " '10.11.2.3',\n",
       " '10.11.2.4',\n",
       " '10.11.2.5',\n",
       " '10.11.2.6',\n",
       " '10.11.3',\n",
       " '10.11.3.4',\n",
       " '10.11.3.5',\n",
       " '10.11.3.6',\n",
       " '10.11.4',\n",
       " '10.2',\n",
       " '10.2.3',\n",
       " '10.2.3.5',\n",
       " '10.2.4',\n",
       " '10.2.5',\n",
       " '10.3',\n",
       " '10.3.4',\n",
       " '11',\n",
       " '11.0',\n",
       " '11.0.1',\n",
       " '11.0.1.2',\n",
       " '11.0.1.2.3.4',\n",
       " '11.0.1.2.4.5.7.9',\n",
       " '11.0.2',\n",
       " '11.0.2.3',\n",
       " '11.0.2.4',\n",
       " '11.0.2.4.5',\n",
       " '11.0.2.4.5.7',\n",
       " '11.0.2.4.5.7.9',\n",
       " '11.0.2.4.7',\n",
       " '11.0.2.5',\n",
       " '11.0.2.5.7',\n",
       " '11.0.2.6',\n",
       " '11.0.2.6.7',\n",
       " '11.0.3',\n",
       " '11.0.3.5.8',\n",
       " '11.0.3.6',\n",
       " '11.0.3.7',\n",
       " '11.0.4',\n",
       " '11.0.4.5',\n",
       " '11.0.4.6',\n",
       " '11.0.4.7',\n",
       " '11.0.5',\n",
       " '11.1',\n",
       " '11.1.2',\n",
       " '11.1.2.4',\n",
       " '11.1.2.4.7',\n",
       " '11.1.2.6',\n",
       " '11.1.3',\n",
       " '11.1.3.4.6.8',\n",
       " '11.1.3.5',\n",
       " '11.1.3.6',\n",
       " '11.1.4',\n",
       " '11.1.4.6',\n",
       " '11.1.4.7',\n",
       " '11.1.5',\n",
       " '11.1.6',\n",
       " '11.2',\n",
       " '11.2.3',\n",
       " '11.2.3.4.5.7',\n",
       " '11.2.3.4.7',\n",
       " '11.2.4',\n",
       " '11.2.4.5',\n",
       " '11.2.4.5.7',\n",
       " '11.2.4.5.7.8',\n",
       " '11.2.4.6',\n",
       " '11.2.4.6.7',\n",
       " '11.2.4.7',\n",
       " '11.2.4.7.8',\n",
       " '11.2.5',\n",
       " '11.2.5.6',\n",
       " '11.2.5.7',\n",
       " '11.2.5.7.8',\n",
       " '11.2.6',\n",
       " '11.3',\n",
       " '11.3.4',\n",
       " '11.3.4.6',\n",
       " '11.3.5',\n",
       " '11.3.5.6',\n",
       " '11.3.6',\n",
       " '11.4',\n",
       " '11.4.5',\n",
       " '2',\n",
       " '2.3',\n",
       " '2.3.4',\n",
       " '2.3.4.5.6',\n",
       " '2.3.4.5.6.7',\n",
       " '2.3.4.5.6.7.8.9.10.11',\n",
       " '2.3.4.5.7',\n",
       " '2.3.4.5.9.10',\n",
       " '2.3.4.6',\n",
       " '2.3.4.6.7',\n",
       " '2.3.4.7',\n",
       " '2.3.4.7.9',\n",
       " '2.3.4.8.9.10',\n",
       " '2.3.5',\n",
       " '2.3.5.7',\n",
       " '2.3.5.7.10',\n",
       " '2.3.5.9',\n",
       " '2.3.6',\n",
       " '2.3.6.7',\n",
       " '2.3.6.8',\n",
       " '2.3.7',\n",
       " '2.3.7.10',\n",
       " '2.3.7.9',\n",
       " '2.3.7.9.10',\n",
       " '2.4',\n",
       " '2.4.5',\n",
       " '2.4.5.6',\n",
       " '2.4.5.7',\n",
       " '2.4.5.7.10',\n",
       " '2.4.5.7.8.9.10.11.0',\n",
       " '2.4.5.7.9',\n",
       " '2.4.5.7.9.11',\n",
       " '2.4.5.9',\n",
       " '2.4.6',\n",
       " '2.4.6.10',\n",
       " '2.4.6.8',\n",
       " '2.4.6.8.10.11',\n",
       " '2.4.6.8.11',\n",
       " '2.4.6.9',\n",
       " '2.4.6.9.11',\n",
       " '2.4.7',\n",
       " '2.4.7.10',\n",
       " '2.4.7.8',\n",
       " '2.4.7.9',\n",
       " '2.4.7.9.10.11',\n",
       " '2.4.8',\n",
       " '2.4.8.10',\n",
       " '2.4.9',\n",
       " '2.5',\n",
       " '2.5.6',\n",
       " '2.5.7',\n",
       " '2.5.7.10',\n",
       " '2.5.7.8',\n",
       " '2.5.7.8.10',\n",
       " '2.5.7.9',\n",
       " '2.5.7.9.10',\n",
       " '2.5.8',\n",
       " '2.5.8.10',\n",
       " '2.5.8.10.11',\n",
       " '2.5.8.11',\n",
       " '2.5.8.9',\n",
       " '2.5.9',\n",
       " '2.6',\n",
       " '2.6.10',\n",
       " '2.6.7',\n",
       " '2.6.7.9',\n",
       " '2.6.8',\n",
       " '2.6.9',\n",
       " '2.7',\n",
       " '2.7.8',\n",
       " '2.8',\n",
       " '3',\n",
       " '3.4',\n",
       " '3.4.5',\n",
       " '3.4.5.6',\n",
       " '3.4.5.6.7.8.9.10',\n",
       " '3.4.5.7.11',\n",
       " '3.4.6',\n",
       " '3.4.6.10',\n",
       " '3.4.6.7',\n",
       " '3.4.6.8.11',\n",
       " '3.4.6.8.9.11.1',\n",
       " '3.4.7',\n",
       " '3.4.8',\n",
       " '3.4.8.10',\n",
       " '3.4.8.11',\n",
       " '3.4.8.9',\n",
       " '3.4.9',\n",
       " '3.4.9.10',\n",
       " '3.5',\n",
       " '3.5.10',\n",
       " '3.5.6',\n",
       " '3.5.6.10',\n",
       " '3.5.7',\n",
       " '3.5.7.10',\n",
       " '3.5.7.10.0',\n",
       " '3.5.7.11',\n",
       " '3.5.7.9',\n",
       " '3.5.8',\n",
       " '3.5.8.10',\n",
       " '3.5.8.11',\n",
       " '3.5.9',\n",
       " '3.5.9.11',\n",
       " '3.6',\n",
       " '3.6.10',\n",
       " '3.6.8',\n",
       " '3.6.8.10',\n",
       " '3.6.8.11',\n",
       " '3.6.8.9',\n",
       " '3.6.9',\n",
       " '3.6.9.11',\n",
       " '3.6.9.11.0',\n",
       " '3.7',\n",
       " '3.7.10',\n",
       " '3.7.11',\n",
       " '3.7.8',\n",
       " '3.7.9',\n",
       " '3.7.9.10',\n",
       " '3.8',\n",
       " '3.8.9',\n",
       " '3.9',\n",
       " '4',\n",
       " '4.10',\n",
       " '4.5',\n",
       " '4.5.10',\n",
       " '4.5.6',\n",
       " '4.5.6.7',\n",
       " '4.5.6.7.8.9',\n",
       " '4.5.7',\n",
       " '4.5.7.11',\n",
       " '4.5.7.8.10.1',\n",
       " '4.5.7.9',\n",
       " '4.5.7.9.0',\n",
       " '4.5.7.9.11',\n",
       " '4.5.7.9.11.0',\n",
       " '4.5.8',\n",
       " '4.5.8.0',\n",
       " '4.5.8.10.1',\n",
       " '4.5.8.11',\n",
       " '4.5.9',\n",
       " '4.5.9.0',\n",
       " '4.5.9.10',\n",
       " '4.5.9.11',\n",
       " '4.6',\n",
       " '4.6.10',\n",
       " '4.6.10.0',\n",
       " '4.6.11',\n",
       " '4.6.7',\n",
       " '4.6.7.10',\n",
       " '4.6.7.9',\n",
       " '4.6.8',\n",
       " '4.6.8.10.1',\n",
       " '4.6.8.11',\n",
       " '4.6.8.11.1',\n",
       " '4.6.8.9.11',\n",
       " '4.6.9',\n",
       " '4.6.9.0',\n",
       " '4.6.9.11',\n",
       " '4.7',\n",
       " '4.7.10',\n",
       " '4.7.10.0',\n",
       " '4.7.11',\n",
       " '4.7.8',\n",
       " '4.7.8.11',\n",
       " '4.7.8.9.10.0',\n",
       " '4.7.9',\n",
       " '4.7.9.0',\n",
       " '4.7.9.10.0',\n",
       " '4.7.9.11',\n",
       " '4.7.9.11.0',\n",
       " '4.8',\n",
       " '4.8.10',\n",
       " '4.8.10.11',\n",
       " '4.8.11',\n",
       " '4.8.11.0',\n",
       " '4.8.9',\n",
       " '4.8.9.10',\n",
       " '4.8.9.11',\n",
       " '4.9',\n",
       " '4.9.10',\n",
       " '5',\n",
       " '5.10',\n",
       " '5.10.11',\n",
       " '5.11',\n",
       " '5.6',\n",
       " '5.6.10',\n",
       " '5.6.10.0',\n",
       " '5.6.10.1',\n",
       " '5.6.10.11',\n",
       " '5.6.11',\n",
       " '5.6.8',\n",
       " '5.6.8.10.1',\n",
       " '5.6.9.0',\n",
       " '5.6.9.11',\n",
       " '5.7',\n",
       " '5.7.0',\n",
       " '5.7.10',\n",
       " '5.7.10.0',\n",
       " '5.7.10.1',\n",
       " '5.7.10.11',\n",
       " '5.7.11',\n",
       " '5.7.11.0',\n",
       " '5.7.8',\n",
       " '5.7.8.0',\n",
       " '5.7.8.10',\n",
       " '5.7.8.10.0.2',\n",
       " '5.7.8.11',\n",
       " '5.7.8.9.11.2',\n",
       " '5.7.9',\n",
       " '5.7.9.0',\n",
       " '5.7.9.0.2',\n",
       " '5.7.9.1',\n",
       " '5.7.9.11',\n",
       " '5.7.9.11.0',\n",
       " '5.7.9.11.2',\n",
       " '5.8',\n",
       " '5.8.0',\n",
       " '5.8.10',\n",
       " '5.8.10.0',\n",
       " '5.8.10.1',\n",
       " '5.8.11',\n",
       " '5.8.11.1',\n",
       " '5.8.9',\n",
       " '5.8.9.0',\n",
       " '5.9',\n",
       " '5.9.0',\n",
       " '5.9.10',\n",
       " '5.9.10.0',\n",
       " '5.9.11',\n",
       " '5.9.11.0',\n",
       " '6',\n",
       " '6.10',\n",
       " '6.10.0',\n",
       " '6.10.0.1',\n",
       " '6.10.1',\n",
       " '6.10.1.2',\n",
       " '6.10.11',\n",
       " '6.10.11.1',\n",
       " '6.11',\n",
       " '6.11.0',\n",
       " '6.7',\n",
       " '6.7.0',\n",
       " '6.7.10',\n",
       " '6.7.10.11',\n",
       " '6.7.10.2',\n",
       " '6.7.11',\n",
       " '6.7.11.0',\n",
       " '6.7.11.1',\n",
       " '6.7.11.1.2',\n",
       " '6.7.11.2',\n",
       " '6.7.8',\n",
       " '6.7.8.9.10',\n",
       " '6.7.9',\n",
       " '6.7.9.0',\n",
       " '6.7.9.0.2',\n",
       " '6.7.9.1',\n",
       " '6.7.9.10',\n",
       " '6.7.9.10.1',\n",
       " '6.7.9.11',\n",
       " '6.7.9.11.2',\n",
       " '6.8',\n",
       " '6.8.0',\n",
       " '6.8.1',\n",
       " '6.8.10',\n",
       " '6.8.10.0',\n",
       " '6.8.10.1',\n",
       " '6.8.11',\n",
       " '6.8.11.1',\n",
       " '6.8.11.2',\n",
       " '6.8.9',\n",
       " '6.8.9.1',\n",
       " '6.8.9.10.0.2',\n",
       " '6.8.9.11.2',\n",
       " '6.9',\n",
       " '6.9.0',\n",
       " '6.9.0.2',\n",
       " '6.9.0.2.3',\n",
       " '6.9.1',\n",
       " '6.9.10',\n",
       " '6.9.10.0',\n",
       " '6.9.10.0.2',\n",
       " '6.9.11',\n",
       " '6.9.11.0',\n",
       " '6.9.11.1',\n",
       " '6.9.11.2',\n",
       " '7',\n",
       " '7.0',\n",
       " '7.0.1',\n",
       " '7.10',\n",
       " '7.10.0',\n",
       " '7.10.0.1',\n",
       " '7.10.0.2',\n",
       " '7.10.0.3',\n",
       " '7.10.0.3.4',\n",
       " '7.10.1',\n",
       " '7.10.1.3',\n",
       " '7.10.11.2',\n",
       " '7.10.2',\n",
       " '7.11',\n",
       " '7.11.0',\n",
       " '7.11.0.2',\n",
       " '7.11.1',\n",
       " '7.11.2',\n",
       " '7.8',\n",
       " '7.8.0',\n",
       " '7.8.0.1',\n",
       " '7.8.0.2',\n",
       " '7.8.0.3',\n",
       " '7.8.1',\n",
       " '7.8.10',\n",
       " '7.8.10.0.2',\n",
       " '7.8.10.0.3',\n",
       " '7.8.11',\n",
       " '7.8.11.0',\n",
       " '7.8.11.1',\n",
       " '7.8.11.2',\n",
       " '7.8.9',\n",
       " '7.8.9.10',\n",
       " '7.8.9.10.11.0',\n",
       " '7.8.9.10.11.0.1.2.3.4.5',\n",
       " '7.8.9.10.11.0.2.3.4.5',\n",
       " '7.8.9.11.0.2.4.5',\n",
       " '7.8.9.2',\n",
       " '7.9',\n",
       " '7.9.0',\n",
       " '7.9.0.2',\n",
       " '7.9.0.3',\n",
       " '7.9.1',\n",
       " '7.9.10',\n",
       " '7.9.10.0',\n",
       " '7.9.10.1',\n",
       " '7.9.10.11',\n",
       " '7.9.10.2',\n",
       " '7.9.11',\n",
       " '7.9.11.0',\n",
       " '7.9.11.0.2',\n",
       " '7.9.11.0.2.4',\n",
       " '7.9.11.0.3',\n",
       " '7.9.11.1',\n",
       " '7.9.11.1.4',\n",
       " '7.9.11.2',\n",
       " '7.9.11.2.3',\n",
       " '7.9.11.2.4',\n",
       " '7.9.11.3',\n",
       " '7.9.2',\n",
       " '8',\n",
       " '8.0',\n",
       " '8.0.1',\n",
       " '8.0.1.3',\n",
       " '8.0.2',\n",
       " '8.0.3',\n",
       " '8.0.3.4',\n",
       " '8.1',\n",
       " '8.1.2',\n",
       " '8.10',\n",
       " '8.10.0',\n",
       " '8.10.0.1',\n",
       " '8.10.0.2.3',\n",
       " '8.10.0.2.5',\n",
       " '8.10.0.3',\n",
       " '8.10.0.4',\n",
       " '8.10.1',\n",
       " '8.10.1.3',\n",
       " '8.10.1.4',\n",
       " '8.10.11',\n",
       " '8.10.11.2',\n",
       " '8.10.11.2.4',\n",
       " '8.10.2',\n",
       " '8.10.3',\n",
       " '8.11',\n",
       " '8.11.0',\n",
       " '8.11.1',\n",
       " '8.11.1.2',\n",
       " '8.11.1.3',\n",
       " '8.11.1.4',\n",
       " '8.11.1.4.5',\n",
       " '8.11.2',\n",
       " '8.11.2.4',\n",
       " '8.11.3',\n",
       " '8.9',\n",
       " '8.9.0',\n",
       " '8.9.0.3',\n",
       " '8.9.1',\n",
       " '8.9.1.2',\n",
       " '8.9.1.3',\n",
       " '8.9.1.3.4',\n",
       " '8.9.1.4',\n",
       " '8.9.10',\n",
       " '8.9.10.11',\n",
       " '8.9.10.11.0.1.2',\n",
       " '8.9.10.2',\n",
       " '8.9.11',\n",
       " '8.9.11.1',\n",
       " '8.9.11.1.4',\n",
       " '8.9.11.2.4',\n",
       " '8.9.11.3',\n",
       " '8.9.11.3.4',\n",
       " '8.9.2',\n",
       " '9',\n",
       " '9.0',\n",
       " '9.0.2',\n",
       " '9.0.2.4',\n",
       " '9.0.2.4.5',\n",
       " '9.0.2.5',\n",
       " '9.0.2.5.6',\n",
       " '9.0.3',\n",
       " '9.0.3.5',\n",
       " '9.0.4',\n",
       " '9.1',\n",
       " '9.1.2',\n",
       " '9.1.2.4',\n",
       " '9.1.3',\n",
       " '9.1.4',\n",
       " '9.10',\n",
       " '9.10.0',\n",
       " '9.10.0.1.4',\n",
       " '9.10.0.2',\n",
       " '9.10.0.2.4',\n",
       " '9.10.0.2.5',\n",
       " '9.10.0.4',\n",
       " '9.10.11',\n",
       " '9.10.11.0.1',\n",
       " '9.10.11.0.1.2.3.4.5.6.7',\n",
       " '9.10.11.1.4.6',\n",
       " '9.10.11.2.4',\n",
       " '9.10.11.2.5',\n",
       " '9.10.2',\n",
       " '9.10.2.3',\n",
       " '9.10.2.4',\n",
       " '9.10.2.5',\n",
       " '9.10.3',\n",
       " '9.11',\n",
       " '9.11.0',\n",
       " '9.11.0.2',\n",
       " '9.11.0.2.4',\n",
       " '9.11.0.2.4.5',\n",
       " '9.11.0.2.5',\n",
       " '9.11.0.4',\n",
       " '9.11.1',\n",
       " '9.11.1.2',\n",
       " '9.11.1.2.3.5',\n",
       " '9.11.1.2.4',\n",
       " '9.11.1.4',\n",
       " '9.11.1.4.6',\n",
       " '9.11.2',\n",
       " '9.11.2.3',\n",
       " '9.11.2.4',\n",
       " '9.11.2.5',\n",
       " '9.11.3',\n",
       " '9.11.4',\n",
       " '9.2',\n",
       " '9.2.3',\n",
       " 'A0',\n",
       " 'A1',\n",
       " 'A2',\n",
       " 'A3',\n",
       " 'A4',\n",
       " 'A5',\n",
       " 'A6',\n",
       " 'B-0',\n",
       " 'B-1',\n",
       " 'B-2',\n",
       " 'B-3',\n",
       " 'B-4',\n",
       " 'B-5',\n",
       " 'B-6',\n",
       " 'B0',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B3',\n",
       " 'B4',\n",
       " 'B5',\n",
       " 'B6',\n",
       " 'B7',\n",
       " 'C#0',\n",
       " 'C#1',\n",
       " 'C#2',\n",
       " 'C#3',\n",
       " 'C#4',\n",
       " 'C#5',\n",
       " 'C#6',\n",
       " 'C#7',\n",
       " 'C#8',\n",
       " 'C-1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C7',\n",
       " 'C8',\n",
       " 'D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'E-1',\n",
       " 'E-2',\n",
       " 'E-3',\n",
       " 'E-4',\n",
       " 'E-5',\n",
       " 'E-6',\n",
       " 'E-7',\n",
       " 'E-8',\n",
       " 'E1',\n",
       " 'E2',\n",
       " 'E3',\n",
       " 'E4',\n",
       " 'E5',\n",
       " 'E6',\n",
       " 'E7',\n",
       " 'E8',\n",
       " 'F#-1',\n",
       " 'F#0',\n",
       " 'F#1',\n",
       " 'F#2',\n",
       " 'F#3',\n",
       " 'F#4',\n",
       " 'F#5',\n",
       " 'F#6',\n",
       " 'F#7',\n",
       " 'F#8',\n",
       " 'F1',\n",
       " 'F2',\n",
       " 'F3',\n",
       " 'F4',\n",
       " 'F5',\n",
       " 'F6',\n",
       " 'G#-1',\n",
       " 'G#0',\n",
       " 'G#1',\n",
       " 'G#2',\n",
       " 'G#3',\n",
       " 'G#4',\n",
       " 'G#5',\n",
       " 'G#6',\n",
       " 'G-1',\n",
       " 'G0',\n",
       " 'G1',\n",
       " 'G2',\n",
       " 'G3',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'G6',\n",
       " 'G7']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"jazz.txt\", 'r') as f:\n",
    "    notes = f.read().split('\\n')\n",
    "\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "pitchnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, sequence_length, num_notes):\n",
    "    # dictionary used to map notes to numbers\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # generate training data: sequence of notes => next note\n",
    "    for i in range(len(notes) - sequence_length):\n",
    "        sequence_in = notes[i: i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    # one-hot encode\n",
    "    network_input = to_categorical(network_input, num_notes)\n",
    "    network_input = np.reshape(network_input, (-1, sequence_length, num_notes))\n",
    "    \n",
    "    network_output = to_categorical(network_output, num_notes)\n",
    "\n",
    "    return network_input, network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "sequence_length = 50\n",
    "num_notes = len(set(notes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding \n",
    "\n",
    "- it gives the model information about the relative position of the elements in a sequence (unlike RNNs, it processes all inputs simultaneously rather than sequentially);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, sequence_length, num_notes, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_notes = num_notes\n",
    "        self.pos_encoding = self.positional_encoding(sequence_length, num_notes)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / np.power(10000., (2 * (i // 2)) / np.float32(d_model))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            np.arange(position)[:, np.newaxis],\n",
    "            np.arange(d_model)[np.newaxis, :],\n",
    "            d_model\n",
    "        )\n",
    "        # Apply sin to even indices in the array; 2i\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        # Apply cos to odd indices in the array; 2i+1\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, sequence_length, num_notes, latent_dim=64, num_heads=4, ff_dim=128, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_notes = num_notes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "\n",
    "    def build_encoder(self):\n",
    "        inputs = Input(shape=(self.sequence_length, self.num_notes))\n",
    "        x = self.transformer_encoder(inputs, self.num_notes, self.num_heads, self.ff_dim)\n",
    "        z_mean = Dense(self.latent_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = Dense(self.latent_dim, name=\"z_log_var\")(x)\n",
    "        return Model(inputs, [z_mean, z_log_var], name=\"encoder\")\n",
    "\n",
    "    def build_decoder(self):\n",
    "        latent_inputs = Input(shape=(self.sequence_length, self.latent_dim,))\n",
    "        x = self.transformer_encoder(latent_inputs, self.latent_dim, self.num_heads, self.ff_dim)\n",
    "        outputs = TimeDistributed(Dense(self.num_notes, activation=\"softmax\"))(x)\n",
    "        return Model(latent_inputs, outputs, name=\"decoder\")\n",
    "\n",
    "    def transformer_encoder(self, inputs, dim, num_heads, ff_dim, dropout=0):\n",
    "        inputs = self.add_positional_encoding(inputs, dim)\n",
    "        x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        x = MultiHeadAttention(\n",
    "            key_dim=dim, num_heads=num_heads, dropout=dropout\n",
    "        )(x, x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        res = x + inputs\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(res)\n",
    "        \n",
    "        x = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "        x = Dense(dim)(x)\n",
    "        out2 = LayerNormalization(epsilon=1e-6)(out1 + x)\n",
    "\n",
    "        return out2\n",
    "\n",
    "    def add_positional_encoding(self, x, dim):\n",
    "        pos_enc = PositionalEncoding(self.sequence_length, dim)\n",
    "        return pos_enc(x)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encoder(inputs)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        eps = tf.random.normal(shape=tf.shape(mean))\n",
    "        return eps * tf.exp(log_var * 0.5 + 1e-7) + mean\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var = self.encoder(data)\n",
    "            z = self.reparameterize(z_mean, z_log_var)\n",
    "            reconstruction = self.decoder(z)  \n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                categorical_crossentropy(data, reconstruction)\n",
    "            )\n",
    "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)  # Removed the clipping operation\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "\n",
    "\n",
    "vae = VAE(sequence_length, num_notes)\n",
    "vae.compile(optimizer=Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50, 787)]    0           []                               \n",
      "                                                                                                  \n",
      " positional_encoding (Positiona  (None, 50, 787)     0           ['input_1[0][0]']                \n",
      " lEncoding)                                                                                       \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 50, 787)     1574        ['positional_encoding[0][0]']    \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 50, 787)     9920135     ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 50, 787)      0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 50, 787)     0           ['dropout[0][0]',                \n",
      " da)                                                              'positional_encoding[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 50, 787)     1574        ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 50, 128)      100864      ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 50, 787)      101523      ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 50, 787)     0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 50, 787)     1574        ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 50, 64)       50432       ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 50, 64)       50432       ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,228,108\n",
      "Trainable params: 10,228,108\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 50, 64)]     0           []                               \n",
      "                                                                                                  \n",
      " positional_encoding_1 (Positio  (None, 50, 64)      0           ['input_2[0][0]']                \n",
      " nalEncoding)                                                                                     \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 50, 64)      128         ['positional_encoding_1[0][0]']  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 50, 64)      66368       ['layer_normalization_3[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 50, 64)       0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 50, 64)      0           ['dropout_1[0][0]',              \n",
      " mbda)                                                            'positional_encoding_1[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 50, 64)      128         ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 50, 128)      8320        ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 50, 64)       8256        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 50, 64)      0           ['layer_normalization_4[0][0]',  \n",
      " mbda)                                                            'dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 50, 64)      128         ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 50, 787)     51155       ['layer_normalization_5[0][0]']  \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 134,483\n",
      "Trainable params: 134,483\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the one-hot encoded dataset into batches with DataGenerator\n",
    "def data_generator(notes, sequence_length, num_notes, batch_size):\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    while True:\n",
    "        for i in range(0, len(notes) - sequence_length, batch_size):\n",
    "            network_input = []\n",
    "            network_output = []\n",
    "            \n",
    "            # input-output sequences batches\n",
    "            for j in range(i, min(i + batch_size, len(notes) - sequence_length)):\n",
    "                sequence_in = notes[j: j + sequence_length]\n",
    "                sequence_out = notes[j + sequence_length]\n",
    "                network_input.append([note_to_int[char] for char in sequence_in])\n",
    "                network_output.append(note_to_int[sequence_out])\n",
    "            \n",
    "            # one-hot encode\n",
    "            network_input = to_categorical(network_input, num_notes)\n",
    "            network_input = np.reshape(network_input, (-1, sequence_length, num_notes))\n",
    "            \n",
    "            network_output = to_categorical(network_output, num_notes)\n",
    "            \n",
    "            yield network_input, network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "generator = data_generator(notes, sequence_length, num_notes=num_notes, batch_size=batch_size)\n",
    "\n",
    "steps_per_epoch = (len(notes) - sequence_length) // batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "9398/9398 [==============================] - 5946s 632ms/step - loss: 4.8395 - reconstruction_loss: 4.7434 - kl_loss: 0.0961\n",
      "Epoch 2/2\n",
      "9398/9398 [==============================] - 5481s 583ms/step - loss: 4.7117 - reconstruction_loss: 4.6439 - kl_loss: 0.0677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e0f294d30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(generator, epochs=2, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TransformerVAE\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TransformerVAE\\assets\n"
     ]
    }
   ],
   "source": [
    "vae.save(\"TransformerVAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = tf.keras.models.load_model(\"TransformerVAE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature hyperparameter is used to control the randomness of the output: trade-off between following the model's predictions (low temperatures) and generating diverse output (high temperatures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model, seed, sequence_length, num_notes, temperature=1.0):\n",
    "    # starting from seed sequence generate new sequence\n",
    "    sequence = list(seed)\n",
    "    sequence = sequence[:sequence_length]\n",
    "    \n",
    "    for i in range(sequence_length): \n",
    "        seed_sequence = sequence[-sequence_length:]\n",
    "        \n",
    "        # one-hot encode input\n",
    "        seed_sequence_one_hot = to_categorical(seed_sequence, num_notes)\n",
    "        seed_sequence_one_hot = np.reshape(seed_sequence_one_hot, (1, sequence_length, num_notes))\n",
    "\n",
    "        output_sequence = model(seed_sequence_one_hot)\n",
    "        \n",
    "        # last note from the output sequence\n",
    "        last_note = output_sequence.numpy()[0, -1, :]\n",
    "        \n",
    "        # apply temperature\n",
    "        prediction = np.log(last_note + 1e-7) / temperature\n",
    "        prediction = np.exp(prediction) / np.sum(np.exp(prediction))\n",
    "        \n",
    "        # next note from the adjusted prediction\n",
    "        if np.isnan(prediction).any():\n",
    "            pass\n",
    "        else:\n",
    "            next_note = np.random.choice(range(num_notes), p=prediction)\n",
    "            sequence.append(next_note)\n",
    "\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[695,\n",
       " 760,\n",
       " 695,\n",
       " 695,\n",
       " 695,\n",
       " 760,\n",
       " 734,\n",
       " 726,\n",
       " 726,\n",
       " 734,\n",
       " 726,\n",
       " 734,\n",
       " 708,\n",
       " 760,\n",
       " 760,\n",
       " 760,\n",
       " 750,\n",
       " 708,\n",
       " 734,\n",
       " 760,\n",
       " 760,\n",
       " 694,\n",
       " 760,\n",
       " 760,\n",
       " 750,\n",
       " 734,\n",
       " 750,\n",
       " 734,\n",
       " 750,\n",
       " 734,\n",
       " 734,\n",
       " 708,\n",
       " 694,\n",
       " 708,\n",
       " 734,\n",
       " 734,\n",
       " 734,\n",
       " 708,\n",
       " 734,\n",
       " 708,\n",
       " 694,\n",
       " 708,\n",
       " 734,\n",
       " 760,\n",
       " 760,\n",
       " 695,\n",
       " 695,\n",
       " 760,\n",
       " 760,\n",
       " 760,\n",
       " 55,\n",
       " 305,\n",
       " 707,\n",
       " 305,\n",
       " 716,\n",
       " 55,\n",
       " 305,\n",
       " 782,\n",
       " 540,\n",
       " 305,\n",
       " 399,\n",
       " 782,\n",
       " 55,\n",
       " 749,\n",
       " 781,\n",
       " 540,\n",
       " 305,\n",
       " 693,\n",
       " 692,\n",
       " 55,\n",
       " 305,\n",
       " 55,\n",
       " 734,\n",
       " 305,\n",
       " 707,\n",
       " 759,\n",
       " 55,\n",
       " 55,\n",
       " 55,\n",
       " 708,\n",
       " 694,\n",
       " 708,\n",
       " 749,\n",
       " 305,\n",
       " 717,\n",
       " 55,\n",
       " 55,\n",
       " 693,\n",
       " 55,\n",
       " 55,\n",
       " 749,\n",
       " 708,\n",
       " 749,\n",
       " 725,\n",
       " 708,\n",
       " 55,\n",
       " 707,\n",
       " 55,\n",
       " 748,\n",
       " 749]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "int_to_note = {i: n for n, i in note_to_int.items()}\n",
    "\n",
    "first_batch = next(generator)\n",
    "seed_sequence_one_hot = first_batch[0][0]\n",
    "\n",
    "# one-hot encoded sequence to integer format\n",
    "seed_sequence = [int_to_note[np.argmax(note)] for note in seed_sequence_one_hot]\n",
    "\n",
    "# note names to integers\n",
    "seed_sequence = [note_to_int[note] for note in seed_sequence]\n",
    "\n",
    "generated_sequence = generate_music(vae, seed_sequence, sequence_length, num_notes=num_notes, temperature=0.3)\n",
    "generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A5',\n",
       " 'F#5',\n",
       " 'A5',\n",
       " 'A5',\n",
       " 'A5',\n",
       " 'F#5',\n",
       " 'D5',\n",
       " 'C5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'F#5',\n",
       " 'F#5',\n",
       " 'F#5',\n",
       " 'E5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'F#5',\n",
       " 'F#5',\n",
       " 'A4',\n",
       " 'F#5',\n",
       " 'F#5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'A4',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'A4',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'F#5',\n",
       " 'F#5',\n",
       " 'A5',\n",
       " 'A5',\n",
       " 'F#5',\n",
       " 'F#5',\n",
       " 'F#5',\n",
       " '0.4.7',\n",
       " '2.6.9',\n",
       " 'B3',\n",
       " '2.6.9',\n",
       " 'C#4',\n",
       " '0.4.7',\n",
       " '2.6.9',\n",
       " 'G3',\n",
       " '7.11.2',\n",
       " '2.6.9',\n",
       " '4.7.11',\n",
       " 'G3',\n",
       " '0.4.7',\n",
       " 'E4',\n",
       " 'G2',\n",
       " '7.11.2',\n",
       " '2.6.9',\n",
       " 'A3',\n",
       " 'A2',\n",
       " '0.4.7',\n",
       " '2.6.9',\n",
       " '0.4.7',\n",
       " 'D5',\n",
       " '2.6.9',\n",
       " 'B3',\n",
       " 'F#4',\n",
       " '0.4.7',\n",
       " '0.4.7',\n",
       " '0.4.7',\n",
       " 'B4',\n",
       " 'A4',\n",
       " 'B4',\n",
       " 'E4',\n",
       " '2.6.9',\n",
       " 'C#5',\n",
       " '0.4.7',\n",
       " '0.4.7',\n",
       " 'A3',\n",
       " '0.4.7',\n",
       " '0.4.7',\n",
       " 'E4',\n",
       " 'B4',\n",
       " 'E4',\n",
       " 'C4',\n",
       " 'B4',\n",
       " '0.4.7',\n",
       " 'B3',\n",
       " '0.4.7',\n",
       " 'E3',\n",
       " 'E4']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_note = dict((number, note) for number, note in enumerate(sorted(set(item for item in notes))))\n",
    "note_sequence = [int_to_note[i] for i in generated_sequence]\n",
    "note_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a midi file from text representation outputted by TransformerVAE\n",
    "def create_midi(prediction_output, output_file):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    for pattern in prediction_output:\n",
    "        try:\n",
    "            if ('.' in pattern) or pattern.isdigit():\n",
    "                notes_in_chord = pattern.split('.')\n",
    "                notes = []\n",
    "                for current_note in notes_in_chord:\n",
    "                    new_note = note.Note(int(current_note))\n",
    "                    new_note.storedInstrument = instrument.Piano()\n",
    "                    notes.append(new_note)\n",
    "                new_chord = chord.Chord(notes)\n",
    "                new_chord.offset = offset\n",
    "                output_notes.append(new_chord)\n",
    "            else:\n",
    "                # print(pattern)\n",
    "                new_note = note.Note()\n",
    "                new_note.pitch = pitch.Pitch(pattern)\n",
    "                new_note.offset = offset\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                output_notes.append(new_note)\n",
    "\n",
    "            offset += 0.5\n",
    "        except Exception as e:\n",
    "            print(f'Error loading pattern: {pattern}: {e}')\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp=output_file)\n",
    "\n",
    "output_file = \"TransformerVAE_generated_music.mid\"\n",
    "create_midi(note_sequence, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
