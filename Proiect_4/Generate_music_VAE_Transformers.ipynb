{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import mido\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras import Model\n",
    "from keras.layers import Dense, Reshape, InputLayer, Embedding, GlobalAveragePooling1D,\\\n",
    "                            Layer, MultiHeadAttention, LayerNormalization, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess MIDI files -> tokenize the notes and chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 46/934 [00:55<20:35,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ./Jazz_midi\\Andy.mid: 1714211658288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 166/934 [03:20<13:19,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ./Jazz_midi\\Chromazone.mid: 1714079026912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 279/934 [05:43<11:33,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ./Jazz_midi\\Fragile.mid: 1714131821280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 281/934 [05:48<20:13,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ./Jazz_midi\\FreedoomOfSpeech.mid: 1714219085008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 298/934 [06:07<16:05,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ./Jazz_midi\\GlasgowKiss.mid: 1714133881808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 321/934 [06:43<12:44,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ./Jazz_midi\\HelpMe.mid: 1714119524656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 609/934 [13:17<21:10,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ./Jazz_midi\\PlayingForTime.mid: 1714174530512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 864/934 [19:04<02:05,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ./Jazz_midi\\WhatsNewInBaltimore.mid: 1714124881680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 934/934 [20:32<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_midi_files(folder_path, output_file):\n",
    "    notes = []\n",
    "\n",
    "    for file in tqdm(glob.glob(os.path.join(folder_path, \"*.mid\"))):\n",
    "        try:\n",
    "            midi = converter.parse(file)\n",
    "            notes_in_file = None\n",
    "\n",
    "            parts = instrument.partitionByInstrument(midi)\n",
    "            if parts:\n",
    "                notes_in_file = parts.parts[0].recurse()\n",
    "            else:\n",
    "                notes_in_file = midi.flat.notes\n",
    "\n",
    "            for element in notes_in_file:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "        except Exception as e:\n",
    "            print(f'Error loading {file}: {e}')\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write('\\n'.join(notes))\n",
    "\n",
    "input_folder = './Jazz_midi/'\n",
    "output_file = 'jazz.txt'\n",
    "preprocess_midi_files(input_folder, output_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the notes -> create a dictionary to map the unique notes and chords to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_note_to_int_mapping(notes):\n",
    "    unique_notes = sorted(set(notes))\n",
    "    return dict((note, number) for number, note in enumerate(unique_notes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(notes, sequence_length=100):\n",
    "    note_to_int = create_note_to_int_mapping(notes)\n",
    "    vocab_size = len(note_to_int)\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    for i in range(len(notes) - sequence_length):\n",
    "        sequence_in = notes[i: i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # Reshape the input and output data\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    network_input = network_input / float(vocab_size)\n",
    "    network_output = to_categorical(network_output)\n",
    "\n",
    "    return network_input, network_output, note_to_int\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build VAE-Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVAE(Model):\n",
    "    def __init__(self, input_shape, latent_dim):\n",
    "        super(SimpleVAE, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            InputLayer(input_shape=input_shape),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(latent_dim * 2),\n",
    "        ])\n",
    "\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            InputLayer(input_shape=(latent_dim,)),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(np.prod(input_shape), activation='sigmoid'),\n",
    "            Reshape(input_shape),\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        mean, logvar = self.encode(inputs)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mean, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(d_ff, activation='relu'),\n",
    "            Dense(d_model),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, logvar = inputs\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * 0.5) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAETransformer(Model):\n",
    "    def __init__(self, input_shape, latent_dim, num_heads, d_ff, num_layers, num_tokens):\n",
    "        super(VAETransformer, self).__init__()\n",
    "\n",
    "        # VAE Encoder\n",
    "        self.encoder = Sequential([\n",
    "            InputLayer(input_shape=input_shape),\n",
    "            LSTM(512, return_sequences=True),\n",
    "            LSTM(256),\n",
    "            Dense(latent_dim * 2),\n",
    "        ])\n",
    "\n",
    "        # Sampling layer\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "        # Transformer Decoder\n",
    "        self.embedding = Embedding(input_dim=num_tokens, output_dim=512)\n",
    "        self.pos_encoding = positional_encoding(num_tokens, 512)\n",
    "        self.transformer_layers = [\n",
    "            TransformerBlock(d_model=512, num_heads=num_heads, d_ff=d_ff)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.decoder_lstm = LSTM(512, return_sequences=True)\n",
    "        self.output_layer = Dense(num_tokens, activation='softmax')\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.embedding(z)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding.output_dim, tf.float32))\n",
    "        x += self.pos_encoding[:, :x.shape[1], :]\n",
    "\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.decoder_lstm(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        mean, logvar = self.encode(inputs)\n",
    "        z = self.sampling((mean, logvar))\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mean, logvar\n",
    "\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: I changed the model, check again the steps from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_recon, mean, logvar, beta=1.0):\n",
    "    recon_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(x, x_recon))\n",
    "    kl_loss = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
    "    return recon_loss + beta * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(input_data, output_data, vae, transformer, epochs=100, batch_size=64):\n",
    "    vae.compile(optimizer=Adam(), loss=vae_loss)\n",
    "    transformer.compile(optimizer=Adam(), loss=CategoricalCrossentropy())\n",
    "\n",
    "    checkpoint_vae = ModelCheckpoint(\"vae_best_model.h5\", save_best_only=True)\n",
    "    checkpoint_transformer = ModelCheckpoint(\"transformer_best_model.h5\", save_best_only=True)\n",
    "\n",
    "    vae.fit(input_data, input_data, epochs=epochs, batch_size=batch_size, callbacks=[checkpoint_vae])\n",
    "    transformer.fit(input_data, output_data, epochs=epochs, batch_size=batch_size, callbacks=[checkpoint_transformer])\n",
    "\n",
    "    vae.save(\"vae_final_model.h5\")\n",
    "    transformer.save(\"transformer_final_model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate music:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(vae, transformer, int_to_note, seed_sequence, sequence_length=100, output_length=500):\n",
    "    generated_notes = []\n",
    "\n",
    "    for i in range(output_length):\n",
    "        seed_input = np.reshape(seed_sequence, (1, sequence_length, 1))\n",
    "        seed_input = seed_input / float(len(int_to_note))\n",
    "\n",
    "        # Use VAE to encode the input sequence\n",
    "        encoded_seed, _, _ = vae(seed_input)\n",
    "        \n",
    "        # Use Transformer to predict the next note based on the encoded seed\n",
    "        prediction = transformer(encoded_seed)\n",
    "\n",
    "        # Sample the predicted note\n",
    "        predicted_note = np.random.choice(range(len(int_to_note)), p=prediction.numpy().ravel())\n",
    "\n",
    "        # Update the seed sequence with the predicted note\n",
    "        seed_sequence = np.append(seed_sequence[1:], predicted_note)\n",
    "\n",
    "        generated_notes.append(int_to_note[predicted_note])\n",
    "\n",
    "    return generated_notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To generate music in a specific genre, use a dataset with midi files in that genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess MIDI files\n",
    "folder_path = \"path/to/your/midi/files\"\n",
    "notes = preprocess_midi_files(folder_path)\n",
    "\n",
    "# Prepare the dataset\n",
    "sequence_length = 100\n",
    "network_input, network_output, note_to_int = prepare_dataset(notes, sequence_length)\n",
    "int_to_note = {i: n for n, i in note_to_int.items()}\n",
    "\n",
    "# Create the VAE and Transformer models\n",
    "input_shape = (sequence_length, 1)\n",
    "latent_dim = 32\n",
    "num_heads = 8\n",
    "ff_dim = 32\n",
    "num_layers = 2\n",
    "\n",
    "vae = SimpleVAE(input_shape, latent_dim)\n",
    "transformer = SimpleTransformer(input_shape, num_heads, ff_dim, num_layers)\n",
    "\n",
    "# Train the models\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "train_models(network_input, network_output, vae, transformer, epochs, batch_size)\n",
    "\n",
    "# Load the best models\n",
    "vae_best = load_model(\"vae_best_model.h5\", custom_objects={\"SimpleVAE\": SimpleVAE})\n",
    "transformer_best = load_model(\"transformer_best_model.h5\", custom_objects={\"SimpleTransformer\": SimpleTransformer})\n",
    "\n",
    "# Generate music\n",
    "seed_sequence = network_input[np.random.randint(0, len(network_input) - 1)]\n",
    "generated_notes = generate_music(vae_best, transformer_best, int_to_note, seed_sequence)\n",
    "\n",
    "# Convert generated notes to a MIDI file\n",
    "output_midi = stream.Stream()\n",
    "\n",
    "for n in generated_notes:\n",
    "    if '.' in n:\n",
    "        notes_in_chord = n.split('.')\n",
    "        chord_notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            chord_notes.append(new_note)\n",
    "        new_chord = chord.Chord(chord_notes)\n",
    "        output_midi.append(new_chord)\n",
    "    else:\n",
    "        new_note = note.Note(n)\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_midi.append(new_note)\n",
    "\n",
    "output_midi.write('midi', fp='generated_music.mid')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
